{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67RBrvkUviuj"
   },
   "source": [
    "<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n",
    "\n",
    "\n",
    "# Ejercicio de clasificación con redes neuronales (antes KNN)\n",
    "\n",
    "Ejemplo de clasificación utilizando redes neuronales para la clasificación de drogadas que debería tomar un pasiente según su historial clínico<br>\n",
    "\n",
    "v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxUi9BoECANR"
   },
   "source": [
    "### Objetivos: \n",
    "*   Preprocesar los datos (descarga, lectura, limplieza y filtrado).\n",
    "*   Conocer la estructura e implementación de las redes neuronales para clasificación múltiple para clasificación de frutas.\n",
    "*   Comparar resultados del algoritmo de clasificación KNN y Redes Neuronales para el dataset de frutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2sSeyEovSw-"
   },
   "outputs": [],
   "source": [
    "#Librerias a implementar\n",
    "import os\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Szo7P_3v00C"
   },
   "source": [
    "# Recolectar datos\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline1.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LY_jQ6eCnMG"
   },
   "source": [
    "### Código de descarga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnVpNGuAvyFi"
   },
   "outputs": [],
   "source": [
    "if os.access('frutas.csv', os.F_OK) is False:\n",
    "    if platform.system() == 'Windows':\n",
    "        !curl https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/frutas.csv > frutas.csv\n",
    "    else:\n",
    "        !wget frutas.csv https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/frutas.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbNSgxdfw0ix"
   },
   "source": [
    "### `fruta.csv`:\n",
    "El dataset **`fruta.csv`** contiene diferentes tipos de frutas las cuales podremos clasificar en tres categorías según su peso y tamaño. Este dataset se creó a partir de:<br> [Dataset source](https://www.kaggle.com/datasets/mjamilmoughal/fruits-with-colors-dataset)\n",
    "\n",
    "- **peso** --> peso en gramos\n",
    "- **ancho** --> ancho en cm\n",
    "- **alto** --> alto en cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHHsGe1Qypde"
   },
   "source": [
    "# Procesar datos\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline2.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvzaKBMbyoiy"
   },
   "outputs": [],
   "source": [
    "# Una vez descargado el archivo en Colab.\n",
    "# Leerlo con Pandas y el método read_csv\n",
    "# Una vez extraida toda la información se almacena en df\n",
    "# A partir de df y el método describe(), mostrará la descripción estadistica básica del archivo que se guardará en des\n",
    "# Crear una fila nueva llamada Nan en el DataFrame  des,\n",
    "# que indica la cantidad de datos tipo Nan que tiene cada columna.\n",
    "# Para crear una nueva fila, se utilizará el operador loc, donde se indica el nombre\n",
    "# de la nueva fila y con que valores se completará.\n",
    "# La información será de los datos faltantes df.isna().sum()\n",
    "# Crear una fila nueva llamada %Nan en el DataFrame des,\n",
    "# Esta fila se completará con los porcentajes de Nan encontrados en cada columna.\n",
    "\n",
    "df = pd.read_csv(\"frutas.csv\")\n",
    "des = df.describe()\n",
    "des.loc['Nan'] = df.isna().sum()\n",
    "des.loc['%Nan'] = (df.isna().mean())*100\n",
    "des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw9HbE88y3wu"
   },
   "outputs": [],
   "source": [
    "# Muestra las 5 primeras filas del DataFrame df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LirgXKpiy8dr"
   },
   "outputs": [],
   "source": [
    "# Cantidad de filas y columnas con shape\n",
    "# En la ubicación 0 corresponde a las filas\n",
    "print('Cantidad de datos en observacion:', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BnzYdlRzBxz"
   },
   "source": [
    "# Explorar datos\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline3.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2w_eW5QI-hf"
   },
   "outputs": [],
   "source": [
    "# Descripción estadistica básica del DataFrame df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yH6oDykAzBMG"
   },
   "outputs": [],
   "source": [
    "# Se accede a la columna \"fruta\" para contar la frecuencia de los valores únicos (Cuenta la cantidad de frutas distintas que Hay en el dataset).\n",
    "# como está repartida las categorias entre las frutas\n",
    "df['fruta'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5cQJRWdzTk3"
   },
   "outputs": [],
   "source": [
    "# Se representa graficamente la frecuencia de las frutas\n",
    "# Esto permite explorar que tan balanceado está el dataset.\n",
    "# sns, alias de Seaborn\n",
    "# countplot(), gráfico de barras\n",
    "# Necesita toda la data\n",
    "# Se especifica la columna a representar, en este caso \"fruta\"\n",
    "sns.countplot(data=df, x=\"fruta\")\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqVWUXPm0op-"
   },
   "source": [
    "Se puede ver que el dataset está bastante balanceado, no habrá una tendencia marcada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u5tqX2WDog_"
   },
   "outputs": [],
   "source": [
    "# Gráfico que muestra los datos discriminados por la columna \"fruta\"\n",
    "sns.pairplot(df, hue= 'fruta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f1b4PBcJAWU"
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con aquellas columnas que podemos entender su relacion con el objetivo:\n",
    "# 'peso', 'ancho', 'alto', 'fruta'\n",
    "# Para acceder a las columnas mencionadas se accede al DataFrame df[] y como \n",
    "# son varias columnas se indican los nombres en una lista\n",
    "# Almacenandose en el DataFrame df_clean\n",
    "df_clean = df[['peso', 'ancho', 'alto', 'fruta']]\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdfLMthm0yyY"
   },
   "source": [
    "#### Normalización de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqEKDZw0KETM"
   },
   "source": [
    "Analizar cual es la distribución de los datos numéricos\n",
    "- peso\n",
    "- ancho\n",
    "- alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJlDmX_F1ksA"
   },
   "outputs": [],
   "source": [
    "# Se representa graficamente la distribución de los pesos de las frutas\n",
    "# sns, alias de Seaborn\n",
    "# displot(), gráfico de distribución\n",
    "# Necesita toda la data\n",
    "# Se especifica la columna a representar, en este caso \"peso\"\n",
    "sns.histplot(data=df_clean, x='peso')\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_X2CcMqdJ7z6"
   },
   "outputs": [],
   "source": [
    "# Se representa graficamente la distribución de los altos de las frutas\n",
    "# sns, alias de Seaborn\n",
    "# displot(), gráfico de distribución\n",
    "# Necesita toda la data\n",
    "# Se especifica la columna a representar, en este caso \"alto\"\n",
    "sns.displot(data=df_clean, x='alto')\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xCksVi5J_V2"
   },
   "outputs": [],
   "source": [
    "# Se representa graficamente la distribución de los anchos de las frutas\n",
    "# sns, alias de Seaborn\n",
    "# displot(), gráfico de distribución\n",
    "# Necesita toda la data\n",
    "# Se especifica la columna a representar, en este caso \"ancho\"\n",
    "sns.displot(data=df_clean, x='ancho')\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d6BDobHKLHy"
   },
   "source": [
    "Se normalizarán los datos de las columnas; peso, ancho y alto, a través del StandardScaler, ya que el rango de datos de cada columna es diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zc6xNfbgKj4y"
   },
   "outputs": [],
   "source": [
    "# Normalización de datos\n",
    "# Se importa la herramienta de sklearn.preprocessing como StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Se crea una copia del DataFrame df_clean a df_norm\n",
    "df_norm = df_clean.copy()\n",
    "\n",
    "# Se crean los objetos; peso_scaler, ancho_scaler y alto_scaler a partir de la clase StandardScaler()\n",
    "peso_scaler = StandardScaler()\n",
    "ancho_scaler = StandardScaler()\n",
    "alto_scaler = StandardScaler()\n",
    "\n",
    "# Del DataFrame normalizado df_norm se emplea el método .loc para editar los datos de las columnas: peso, ancho e alto \n",
    "# Cada columna se completará con los datos normalizados\n",
    "# Para ello, se utiliza cada objeto creado y accede al método .fit_transform()\n",
    "# se indica la columna del DataFrame a normalizar \n",
    "# Al agregar .values, solo toma los valores, sin nombres de funciones (Los nombres de las columnas).\n",
    "df_norm.loc[:, 'peso'] = peso_scaler.fit_transform(df[['peso']].values)\n",
    "df_norm.loc[:, 'ancho'] = ancho_scaler.fit_transform(df[['ancho']].values)\n",
    "df_norm.loc[:, 'alto'] = alto_scaler.fit_transform(df[['alto']].values)\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aODNf0SoPyK"
   },
   "outputs": [],
   "source": [
    "# Se importa la herramienta LabelEncoder de la librería sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Se crea el objeto le a partir de la clase Label Encoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Se hace una copia de df_norm a df_enc\n",
    "df_enc = df_norm.copy()\n",
    "\n",
    "# Reemplazar la columna de salida de texto a números\n",
    "df_enc['fruta'] = le.fit_transform(df_enc['fruta'])\n",
    "\n",
    "# Observar las primeras 5 filas\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBEXeFsfFyt_"
   },
   "outputs": [],
   "source": [
    "# Clases identificadas\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z_SuZlj3gbQ"
   },
   "source": [
    "# Entrenar modelo\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline4.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntY84fHj3q5q"
   },
   "source": [
    "El primer paso es obtener los datos que serán la entrada del sistema (X) y los datos que serán la salida del modelo estimador (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIg2_OQ43fqZ"
   },
   "outputs": [],
   "source": [
    "# Obtener los valores de X e y\n",
    "# En X se almacenarán todos los valores de las columnas (con .values), excepto los valores de la columna \"fruta\",\n",
    "# la cuál se elimina con el método .drop()\n",
    "# Necesita el nombre de la columna y\n",
    "# axis=1 para que se elimine por filas.\n",
    "X = df_enc.drop('fruta', axis=1).values\n",
    "\n",
    "# En y, sólo se almacena los valores de la columna \"target\", que será la columna objetivo.\n",
    "# Importante, se implementa to_categorical para obterner la información en un array de matrices, donde cada matriz contiene 3 valores, \n",
    "# los tres valores de las categorias y que también representen las mismas cantidad de filas, es similar al onehotencoder.\n",
    "# Necesita indicar la columna \"fruta\" del DataFrame df_enc usando corchetes.\n",
    "# se implementa el método values para obtener solo los valores y que no vengan incluidos los nombres de las columnas.\n",
    "y = to_categorical(df_enc['fruta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mjc3VBr4MeX4"
   },
   "outputs": [],
   "source": [
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUMai--jtV6l"
   },
   "outputs": [],
   "source": [
    "# Entrada a la red neuronal\n",
    "# El [1] indica que solo toma en cuenta el número de columnas\n",
    "in_shape = X.shape[1]\n",
    "in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54jBNa7qtXgz"
   },
   "outputs": [],
   "source": [
    "# Salida a la red neuronal\n",
    "# Que puede tener cualquier valor de la categoría 0,1,2 (manzana, mandarina, limón)\n",
    "out_shape = y.shape[1]\n",
    "out_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbr-SnON4LuM"
   },
   "source": [
    "Siguiente paso es dividir el dataset en entrenamiento (train) y evaluación (test). Utilizaremos el criterio 80%20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVD4YkjS4MW2"
   },
   "outputs": [],
   "source": [
    "# Se importa la herramienta de sklearn.model_selectionl como train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fijamos un \"random_state\" constante para que siempre el dataset se parta de la misma forma\n",
    "# para poder repetir los ensayos\n",
    "# Ojo! Los dataset de train y test son array numpy\n",
    "# Se importa la herramienta de la libreria  train_test_split()\n",
    "# Necesita los valores de X e y\n",
    "# test_size=0.2, permite indicar el porcentaje de valores para validar, equivalente a un 20%\n",
    "# random_state=42,  es un número fijo que utilizan comunmente en documentación, significa que para cada ejecución del algoritmo \n",
    "#se genere nuevos valores aleatorios\n",
    "# y los conjuntos de datos de entrenamiento y pruebas serán diferentes.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fu1u9JhXq9Dy"
   },
   "outputs": [],
   "source": [
    "# Se importa Dense de la librería tensorflow.keras.layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_model(hidden_neurons):\n",
    "    '''Función que recibe hidden_neurons (cantidad de neuronas). Esta \n",
    "    función tiene el objetivo crea un modelo con redes neuronales para \n",
    "    clasificación múltiple'''\n",
    "    \n",
    "    # Se crea el objeto model2 a partir de la clase Sequential()\n",
    "    model = Sequential()\n",
    "\n",
    "    # Crear la capa de entrada y la capa oculta (hidden) de la red, que tendrá:\n",
    "    # --> tantas entradas (input_shape) como columnas (in_shape)\n",
    "    # --> tantas neuronas como deseemos\n",
    "    # --> utilizamos \"sigmoid\" como capa de activación\n",
    "    model.add(Dense(units=hidden_neurons, activation='sigmoid', input_shape=(in_shape,)))\n",
    "\n",
    "    # Crear la capa de salida, que tendrá tantas neuronas como salidas posibles\n",
    "    # Se implementa 'sigmoid' ya que la salida es multiple\n",
    "    model.add(Dense(units=out_shape, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_89g3dSm2wf"
   },
   "outputs": [],
   "source": [
    "# Se invoca a la función que se encarga de crear el modelos con los valores pasados.\n",
    "model = create_model(16)\n",
    "\n",
    "# Configuración del modelo para el entrenamiento, implementando el método compile a partir del modelo creado.\n",
    "# Se necesita indicar los parámetros:\n",
    "# optimizer, nombre del optimizador (es el algoritmo que se encarga del descenso de gradiente estocástico)\n",
    "# Fuente: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "# loss, se llama función de pérdida, representa las categorías conocidas de las predicción. Al ser 'categorical_crossentropy' \n",
    "#la predicción tiene una salida con varias opciones.\n",
    "# metrics, se define la métrica que evaluará el modelo durante el entrenamiento y las pruebas.\n",
    "# learning_rate, tasa de aprendizaje. El valor predeterminado es 0,001.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Se entrena el modelo con el método fit\n",
    "# Necesita definir los valores para X_train, y_train sumado a la cantidad de épocas que seria la iteraciones de entrenamiento y el porcentaje\n",
    "# dirigido a validación (validation_split=0.2)\n",
    "# batch_size, tamaño del lote a entrenar.\n",
    "history = model.fit(X_train, y_train, validation_split=0.2 , epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDuagYJHvNlm"
   },
   "outputs": [],
   "source": [
    "# Variable epocas_conteo, que almacena en una lista la cantidad de épocas de train\n",
    "# history, es la variable que almacena las predicciones del modelo\n",
    "# y de ella se puede acceder a información como su historial (history) del accuracy\n",
    "epocas_conteo= range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "# De Seaborn (sns) se accede al gráfico de línea para representar;\n",
    "# Por un lado, el 'accuracy',\n",
    "# Por el otro, la validación (val_accuracy)\n",
    "sns.lineplot(x=epocas_conteo,  y=history.history['accuracy'], label='train')\n",
    "sns.lineplot(x=epocas_conteo,  y=history.history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL0_TMz7Em4o"
   },
   "outputs": [],
   "source": [
    "# Variable y_hat_prob que almacena las probabilidades de las predicciones\n",
    "# con los datos de evaluación\n",
    "y_hat_prob = model.predict(X_test)\n",
    "\n",
    "# y_hat almacena los índices de los valores (probabilidades) máximos a lo largo de un eje.\n",
    "y_hat = np.argmax(y_hat_prob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUqp47r5hOCv"
   },
   "outputs": [],
   "source": [
    "# De model creado se puede acceder al sumario que muestra la estructura del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3IfjUuI4XnD"
   },
   "source": [
    "# Validar modelo\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline5.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnXeXHwdyHVx"
   },
   "outputs": [],
   "source": [
    "# Calcular la exactitud (accuracy)\n",
    "#scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMKONtv55zL8"
   },
   "outputs": [],
   "source": [
    "# Calcular la exactitud (accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test.argmax(axis=1), y_hat, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeLeYLYz6ZhO"
   },
   "outputs": [],
   "source": [
    "# Se utiliza la matriz de confusión para evaluar la precisión de una clasificación.\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Necesita dos variables que contengan los valores a comparar\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_hat)\n",
    "\n",
    "# Código para realizar la representación gráfica con los resultados\n",
    "# Se crea la varible cmd, que almacena visualization de la Confusion Matrix \n",
    "# Necesita la variable cm que contiene los resultados de la comparación entre los valores reales y predicción\n",
    "# display_labels, se especifica las etiquetas de las categorias que se evalúan.\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
    "\n",
    "# Con cmd.plot se especifica el mapa de colores reconocido por matplotlib.\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dZxGbjG96jR"
   },
   "source": [
    "# Utilizar modelo\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline6.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6euFUIFDRbmP"
   },
   "outputs": [],
   "source": [
    "# Supongamos que deseamos ver a que categoría que\n",
    "# pertenece una fruta con las siguientes características\n",
    "peso = 130 \n",
    "ancho = 15\n",
    "alto = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e01cqkzTS8n3"
   },
   "outputs": [],
   "source": [
    "# El scaler espera como entrada una matriz (filas y columnas)\n",
    "# Por eso el doble corchete\n",
    "peso_numpy = np.array([[peso]])\n",
    "\n",
    "# Utilizamos float para convertir la matriz que retorna el scaler\n",
    "# a un número\n",
    "peseo_norm = float(peso_scaler.transform(peso_numpy))\n",
    "ancho_norm = float(ancho_scaler.transform(np.array([[ancho]])))\n",
    "alto_norm = float(alto_scaler.transform(np.array([[alto]])))\n",
    "\n",
    "# El sistema espera como entrada \"X\" en este caso una sola fila pero varias\n",
    "# columnas, por eso el reshape(1, -1) donde el \"-1\" significa \"varias\"\n",
    "# (el sistema determina cuantas)\n",
    "X_prueba = np.array([peseo_norm, ancho_norm, alto_norm]).reshape(1, -1)\n",
    "print('Shape:', X_prueba.shape)\n",
    "print('Valores:\\n', X_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd57KsgVGRsj"
   },
   "outputs": [],
   "source": [
    "# Se utiliza el método .predict() para predecir\n",
    "mi_categoria_probabilidades = model.predict(X_prueba)\n",
    "\n",
    "# Muestra las probabilidades para cada categoría\n",
    "mi_categoria_probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IJqDFQ1qUNT"
   },
   "outputs": [],
   "source": [
    "# mi_categoria almacena los índices de los valores (probabilidades) máximos a lo largo de un eje.\n",
    "mi_categoria = mi_categoria_probabilidades.argmax()\n",
    "\n",
    "# Devuelve la ubicación de la probabilidad más alta para los valores de prueba\n",
    "mi_categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMMzBQugHNiM"
   },
   "outputs": [],
   "source": [
    "# Una vez conocida la ubicación de la probabilidad más alta\n",
    "# Con le.inverse_transform se ubica el nombre de la categoría con probabilidad más alta\n",
    "category = le.inverse_transform([mi_categoria])\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUmJ7tFUHuG7"
   },
   "outputs": [],
   "source": [
    "# Del array array(['manzana']) accedemos a la primer ubicación \n",
    "category[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7yzVZcZ9-4m"
   },
   "source": [
    "# Conclusión\n",
    "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline7.png\" width=\"1000\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWAReOgo-B7b"
   },
   "source": [
    "En este ejemplo se obtuvo muy poca performance, pero es una herramienta que también se puede implementar para comparar resultados, como por ejemplo; el caso del algoritmo de clasificación KNN para este mismo dataset donde el accuracy obtenido fue del 94% a diferencia de las redes neuronales donde se obtuvo un 82%."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "97e12f1ac9f26099b54b9e7d4ebc9834a88f2d401bac5ba802dc7f7f5939f7f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
